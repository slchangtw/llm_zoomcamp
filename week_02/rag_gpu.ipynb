{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d658f909-e679-41e9-9c4e-e0241c719049",
   "metadata": {},
   "source": [
    "If you're not running in Saturn Cloud, you need to install these libraries:\n",
    "\n",
    "Make sure you use the latest versions\n",
    "\n",
    "```\n",
    "pip install -U transformers accelerate bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b487d98e-65ab-45d1-8499-4c94f431ea06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:13:14.148728Z",
     "iopub.status.busy": "2024-07-07T14:13:14.148010Z",
     "iopub.status.idle": "2024-07-07T14:13:17.663872Z",
     "shell.execute_reply": "2024-07-07T14:13:17.663227Z",
     "shell.execute_reply.started": "2024-07-07T14:13:14.148680Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -qqq transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b52b09-64e9-4b83-ba6e-442c02be542a",
   "metadata": {},
   "source": [
    "If you're running in Saturn Cloud, change the HuggingFace model folder to '/run/cache/' to ensure you have enough space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60c949d-55da-411e-9ac5-21d7a5535457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:13:17.668125Z",
     "iopub.status.busy": "2024-07-07T14:13:17.667870Z",
     "iopub.status.idle": "2024-07-07T14:13:17.672234Z",
     "shell.execute_reply": "2024-07-07T14:13:17.671454Z",
     "shell.execute_reply.started": "2024-07-07T14:13:17.668100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/run/cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18a31e-39b7-4558-8adc-542c2e10a241",
   "metadata": {},
   "source": [
    "Download `minsearch.py` to create a document search engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22e613d-78df-43f8-816a-04e1ae3629ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:13:17.740255Z",
     "iopub.status.busy": "2024-07-07T14:13:17.739953Z",
     "iopub.status.idle": "2024-07-07T14:13:19.116912Z",
     "shell.execute_reply": "2024-07-07T14:13:19.116152Z",
     "shell.execute_reply.started": "2024-07-07T14:13:17.740231Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-07 14:13:18--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py’\n",
      "\n",
      "minsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-07-07 14:13:19 (44.3 MB/s) - ‘minsearch.py’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -f minsearch.py\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a72eab-4be8-4e29-87fa-3179e46eebcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:13:19.118698Z",
     "iopub.status.busy": "2024-07-07T14:13:19.118348Z",
     "iopub.status.idle": "2024-07-07T14:13:24.101982Z",
     "shell.execute_reply": "2024-07-07T14:13:24.101177Z",
     "shell.execute_reply.started": "2024-07-07T14:13:19.118661Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /run/cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    pipeline,\n",
    ")\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "import minsearch\n",
    "\n",
    "login(token=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd083a-fe4c-4113-9c2d-da7fb1e9ced8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:48:29.179994Z",
     "iopub.status.busy": "2024-07-07T08:48:29.179517Z",
     "iopub.status.idle": "2024-07-07T08:48:29.183206Z",
     "shell.execute_reply": "2024-07-07T08:48:29.182568Z",
     "shell.execute_reply.started": "2024-07-07T08:48:29.179967Z"
    },
    "tags": []
   },
   "source": [
    "# Create a document search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac947de-effd-4b61-8792-a6d7a133f347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:13:24.104169Z",
     "iopub.status.busy": "2024-07-07T14:13:24.103751Z",
     "iopub.status.idle": "2024-07-07T14:13:24.562978Z",
     "shell.execute_reply": "2024-07-07T14:13:24.562131Z",
     "shell.execute_reply.started": "2024-07-07T14:13:24.104145Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7f45741b3a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_url = \"https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1\"\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course[\"course\"]\n",
    "\n",
    "    for doc in course[\"documents\"]:\n",
    "        doc[\"course\"] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"], keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd99c2-7f1d-4512-ab3f-e25b3127427c",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567e78cb-a05f-46b6-a3a8-616d5e3b75e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:13:24.564338Z",
     "iopub.status.busy": "2024-07-07T14:13:24.563958Z",
     "iopub.status.idle": "2024-07-07T14:13:24.579247Z",
     "shell.execute_reply": "2024-07-07T14:13:24.578569Z",
     "shell.execute_reply.started": "2024-07-07T14:13:24.564315Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_gpu_memory() -> None:\n",
    "    global tokenizer\n",
    "    global model\n",
    "\n",
    "    if \"tokenizer\" in globals():\n",
    "        del tokenizer\n",
    "    if \"model\" in globals():\n",
    "        del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def get_tokenizer_model(model_name: str):\n",
    "    clear_gpu_memory()  # Release allocated memory\n",
    "\n",
    "    if model_name == \"google/flan-t5-xl\":\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\n",
    "            model_name, device_map=\"auto\"\n",
    "        )\n",
    "    elif model_name == \"mistralai/Mistral-7B-v0.1\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, device_map=\"auto\", load_in_4bit=True\n",
    "        )\n",
    "    elif model_name == \"microsoft/Phi-3-mini-128k-instruct\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"The model is not supported.\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def search_docs(\n",
    "    query: str, course: str, num_results: int = 3, boost: Optional[dict] = None\n",
    "):\n",
    "    if not boost:\n",
    "        boost = {\"question\": 3.0, \"section\": 0.5}\n",
    "\n",
    "    docs = index.search(\n",
    "        query=query,\n",
    "        filter_dict={\"course\": course},\n",
    "        boost_dict=boost,\n",
    "        num_results=num_results,\n",
    "    )\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_prompt(\n",
    "    question: str, response_docs: list[dict[str, str]], model_name: str\n",
    ") -> str:\n",
    "    if model_name == \"mistralai/Mistral-7B-v0.1\":\n",
    "        prompt_template = \"\"\"\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\".strip()\n",
    "        context = \"\"\n",
    "        for doc in response_docs:\n",
    "            context += f\"{doc['question']}\\n{doc['text']}\\n\\n\"\n",
    "    else:\n",
    "        prompt_template = \"\"\"\n",
    "    You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database. Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "    \"\"\".strip()\n",
    "        context = \"\"\n",
    "        for doc in response_docs:\n",
    "            context += f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "\n",
    "    return prompt_template.format(question=question, context=context)\n",
    "\n",
    "\n",
    "def llm(prompt: str, model_name: str, tokenizer, model, generation_params=None) -> str:\n",
    "    if generation_params is None:\n",
    "        generation_params = {}\n",
    "\n",
    "    if model_name == \"google/flan-t5-xl\":\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=generation_params.get(\"max_length\", 100),\n",
    "            num_beams=generation_params.get(\"num_beams\", 5),\n",
    "            do_sample=generation_params.get(\"do_sample\", False),\n",
    "            temperature=generation_params.get(\"temperature\", 1.0),\n",
    "            top_k=generation_params.get(\"top_k\", 50),\n",
    "            top_p=generation_params.get(\"top_p\", 0.95),\n",
    "        )\n",
    "        output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    elif model_name == \"mistralai/Mistral-7B-v0.1\":\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        outputs = pipe(prompt, **generation_params)\n",
    "        output = outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
    "    elif model_name == \"microsoft/Phi-3-mini-128k-instruct\":\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        outputs = pipe(messages, **generation_params)\n",
    "        output = outputs[0][\"generated_text\"].strip()\n",
    "    else:\n",
    "        raise ValueError(\"The model is not supported.\")\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def rag(\n",
    "    query: str, course: str, model_name: str, tokenizer, model, generation_params=None\n",
    "):\n",
    "    search_results = search_docs(query, course=course)\n",
    "    prompt = build_prompt(query, search_results, model_name)\n",
    "    return llm(prompt, model_name, tokenizer, model, generation_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b774d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:13:24.580850Z",
     "iopub.status.busy": "2024-07-07T14:13:24.580537Z",
     "iopub.status.idle": "2024-07-07T14:13:24.585952Z",
     "shell.execute_reply": "2024-07-07T14:13:24.585385Z",
     "shell.execute_reply.started": "2024-07-07T14:13:24.580826Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"I just discovered this course. Can I still join it?\"\n",
    "course = \"data-engineering-zoomcamp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e22d8-0ddf-4802-9514-8c812e8fd43f",
   "metadata": {},
   "source": [
    "# Model: google/flan-t5-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb1212f-b32c-4a7f-8d16-95f1dcffbf23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:13:25.917852Z",
     "iopub.status.busy": "2024-07-07T14:13:25.917468Z",
     "iopub.status.idle": "2024-07-07T14:14:57.301871Z",
     "shell.execute_reply": "2024-07-07T14:14:57.301214Z",
     "shell.execute_reply.started": "2024-07-07T14:13:25.917827Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c30f2ba8c7404fa5c6ea05d266e436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-xl\"\n",
    "tokenizer, model = get_tokenizer_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09a4005f-1b23-4204-aa1d-a666ce4a5c0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:14:57.303377Z",
     "iopub.status.busy": "2024-07-07T14:14:57.303004Z",
     "iopub.status.idle": "2024-07-07T14:15:01.533849Z",
     "shell.execute_reply": "2024-07-07T14:15:01.533027Z",
     "shell.execute_reply.started": "2024-07-07T14:14:57.303352Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(query, course, model_name, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef0a46-905a-4a9a-a2c9-21f14f7bb8e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T12:52:39.627018Z",
     "iopub.status.busy": "2024-07-07T12:52:39.626626Z",
     "iopub.status.idle": "2024-07-07T12:52:39.630501Z",
     "shell.execute_reply": "2024-07-07T12:52:39.629665Z",
     "shell.execute_reply.started": "2024-07-07T12:52:39.626991Z"
    },
    "tags": []
   },
   "source": [
    "# Model: microsoft/Phi-3-mini-128k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c61de9e4-f45c-4e26-ab55-ea121f010ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:15:01.535185Z",
     "iopub.status.busy": "2024-07-07T14:15:01.534840Z",
     "iopub.status.idle": "2024-07-07T14:16:04.022020Z",
     "shell.execute_reply": "2024-07-07T14:16:04.021325Z",
     "shell.execute_reply.started": "2024-07-07T14:15:01.535160Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb68f4c5b4d49708c5e834f9a4b742c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "tokenizer, model = get_tokenizer_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "431aa74a-2041-4c99-9f83-81c9f0ec3314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:16:04.023840Z",
     "iopub.status.busy": "2024-07-07T14:16:04.023512Z",
     "iopub.status.idle": "2024-07-07T14:16:08.740431Z",
     "shell.execute_reply": "2024-07-07T14:16:08.739643Z",
     "shell.execute_reply.started": "2024-07-07T14:16:04.023814Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yes, you can still join the course even if you discover it after the start date. You are eligible to submit homeworks even if you don't register. However, there are deadlines for final projects, so it's important not to wait until the last minute.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_params = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "rag(query, course, model_name, tokenizer, model, generation_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5af9bd-062e-48cf-97e7-1614f2d88bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T13:19:35.930925Z",
     "iopub.status.busy": "2024-07-07T13:19:35.930400Z",
     "iopub.status.idle": "2024-07-07T13:19:35.934312Z",
     "shell.execute_reply": "2024-07-07T13:19:35.933510Z",
     "shell.execute_reply.started": "2024-07-07T13:19:35.930894Z"
    },
    "tags": []
   },
   "source": [
    "# Model: mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7f9b20f-8789-49e1-90d6-e772ff9cad59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:16:08.742442Z",
     "iopub.status.busy": "2024-07-07T14:16:08.742009Z",
     "iopub.status.idle": "2024-07-07T14:18:01.088252Z",
     "shell.execute_reply": "2024-07-07T14:18:01.087503Z",
     "shell.execute_reply.started": "2024-07-07T14:16:08.742416Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788c01a8210a4596897695e603e7a3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer, model = get_tokenizer_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34afa671-b981-4511-abb2-5198c6939088",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T14:18:01.089621Z",
     "iopub.status.busy": "2024-07-07T14:18:01.089289Z",
     "iopub.status.idle": "2024-07-07T14:18:42.824639Z",
     "shell.execute_reply": "2024-07-07T14:18:42.823885Z",
     "shell.execute_reply.started": "2024-07-07T14:18:01.089596Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, you can still join the course.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_params = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"num_return_sequences\": 1,\n",
    "}\n",
    "\n",
    "rag(query, course, model_name, tokenizer, model, generation_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed636f37-04e5-4100-8ab0-a8892a0da66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
